<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://prometheus.io/</id>
  <title>Prometheus Blog</title>
  <updated>2015-06-01T04:00:00Z</updated>
  <link rel="alternate" href="http://prometheus.io/"/>
  <link rel="self" href="http://prometheus.io/blog/feed.xml"/>
  <author>
    <name>© Prometheus Authors 2015</name>
    <uri>http://prometheus.io/blog/</uri>
  </author>
  <entry>
    <id>tag:prometheus.io,2015-06-01:/blog/2015/06/01/advanced-service-discovery/</id>
    <title type="html">Advanced Service Discovery in Prometheus 0.14.0</title>
    <published>2015-06-01T04:00:00Z</published>
    <updated>2015-06-01T04:00:00Z</updated>
    <author>
      <name>Fabian Reinartz, Julius Volz</name>
      <uri>http://prometheus.io/blog/</uri>
    </author>
    <link rel="alternate" href="http://prometheus.io/blog/2015/06/01/advanced-service-discovery/"/>
    <content type="html">&lt;p&gt;This week we released Prometheus v0.14.0 — a version with many long-awaited additions
and improvements.&lt;/p&gt;

&lt;p&gt;On the user side, Prometheus now supports new service discovery mechanisms. In
addition to DNS-SRV records, it now supports &lt;a href="https://www.consul.io"&gt;Consul&lt;/a&gt;
out of the box, and a file-based interface allows you to connect your own
discovery mechanisms. Over time, we plan to add other common service discovery
mechanisms to Prometheus.&lt;/p&gt;

&lt;p&gt;Aside from many smaller fixes and improvements, you can now also reload your configuration during
runtime by sending a &lt;code&gt;SIGHUP&lt;/code&gt; to the Prometheus process. For a full list of changes, check the
&lt;a href="https://github.com/prometheus/prometheus/blob/master/CHANGELOG.md#0140--2015-06-01"&gt;changelog for this release&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we will take a closer look at the built-in service discovery mechanisms and provide
some practical examples. As an additional resource, see
&lt;a href="/docs/operating/configuration"&gt;Prometheus's configuration documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="prometheus-and-targets"&gt;Prometheus and targets&lt;a class="header-anchor" href="#prometheus-and-targets" name="prometheus-and-targets"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;For a proper understanding of this blog post, we first need to take a look at how
Prometheus labels targets.&lt;/p&gt;

&lt;p&gt;There are various places in the configuration file where target labels may be
set. They are applied in the following order, with later stages overwriting any
labels set by an earlier stage:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Global labels, which are assigned to every target scraped by the Prometheus instance.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;job&lt;/code&gt; label, which is configured as a default value for each scrape configuration.&lt;/li&gt;
&lt;li&gt;Labels that are set per target group within a scrape configuration.&lt;/li&gt;
&lt;li&gt;Advanced label manipulation via &lt;a href="/docs/operating/configuration/#relabeling-relabel_config"&gt;&lt;em&gt;relabeling&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each stage overwrites any colliding labels from the earlier stages. Eventually, we have a flat
set of labels that describe a single target. Those labels are then attached to every time series that
is scraped from this target.&lt;/p&gt;

&lt;p&gt;Note: Internally, even the address of a target is stored in a special
&lt;code&gt;__address__&lt;/code&gt; label. This can be useful during advanced label manipulation
(relabeling), as we will see later. Labels starting with &lt;code&gt;__&lt;/code&gt; do not appear in
the final time series.&lt;/p&gt;

&lt;h2 id="scrape-configurations-and-relabeling"&gt;Scrape configurations and relabeling&lt;a class="header-anchor" href="#scrape-configurations-and-relabeling" name="scrape-configurations-and-relabeling"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Aside from moving from an ASCII protocol buffer format to YAML, a fundamental change to
Prometheus's configuration is the change from per-job configurations to more generalized scrape
configurations. While the two are almost equivalent for simple setups, scrape configurations
allow for greater flexibility in more advanced use cases.&lt;/p&gt;

&lt;p&gt;Each scrape configuration defines a job name which serves as a default value for the
&lt;code&gt;job&lt;/code&gt; label. The &lt;code&gt;job&lt;/code&gt; label can then be redefined for entire target groups or individual targets.
For example, we can define two target groups, each of which defines targets for one job.
To scrape them with the same parameters, we can configure them as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
- job_name: 'overwritten-default'

  scrape_interval: 10s
  scrape_timeout:  5s

  target_groups:
  - targets: ['10.1.200.130:5051', '10.1.200.134:5051']
    labels:
      job: 'job1'

  - targets: ['10.1.200.130:6220', '10.1.200.134:6221']
    labels:
      job: 'job2'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Through a mechanism named &lt;a href="http://prometheus.io/docs/operating/configuration/#relabeling-relabel_config"&gt;&lt;em&gt;relabeling&lt;/em&gt;&lt;/a&gt;,
any label can be removed, created, or modified on a per-target level. This
enables fine-grained labeling that can also take into account metadata coming
from the service discovery. Relabeling is the last stage of label assignment
and overwrites any labels previously set.&lt;/p&gt;

&lt;p&gt;Relabeling works as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A list of source labels is defined.&lt;/li&gt;
&lt;li&gt;For each target, the values of those labels are concatenated with a separator.&lt;/li&gt;
&lt;li&gt;A regular expression is matched against the resulting string.&lt;/li&gt;
&lt;li&gt;A new value based on those matches is assigned to another label.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mutiple relabeling rules can be defined for each scrape configuration. A simple one
that squashes two labels into one, looks as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;relabel_configs:
- source_labels: ['label_a', 'label_b']
  separator:     ';'
  regex:         '(.*);(.*)'
  replacement:   '${1}-${2}'
  target_label:  'label_c'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This rule transforms a target with the label set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  "job": "job1",
  "label_a": "foo",
  "label_b": "bar"
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;...into a target with the label set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  "job": "job1",
  "label_a": "foo",
  "label_b": "bar",
  "label_c": "foo-bar"
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could then also remove the source labels in an additional relabeling step.&lt;/p&gt;

&lt;p&gt;You can read more about relabeling and how you can use it to filter targets in the
&lt;a href="/docs/operating/configuration#relabeling-relabel_config"&gt;configuration documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Over the next sections, we will see how you can leverage relabeling when using service discovery.&lt;/p&gt;

&lt;h2 id="discovery-with-dns-srv-records"&gt;Discovery with DNS-SRV records&lt;a class="header-anchor" href="#discovery-with-dns-srv-records" name="discovery-with-dns-srv-records"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Since the beginning, Prometheus has supported target discovery via DNS-SRV records.
The respective configuration looked like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;job {
  name: "api-server"
  sd_name: "telemetry.eu-west.api.srv.example.org"
  metrics_path: "/metrics"
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheus 0.14.0 allows you to specify multiple SRV records to be queried in a
single scrape configuration, and also provides service-discovery-specific meta
information that is helpful during the relabeling phase.&lt;/p&gt;

&lt;p&gt;When querying the the DNS-SRV records, a label named &lt;code&gt;__meta_dns_srv_name&lt;/code&gt; is
attached to each target. Its value is set to the SRV record name for which it was
returned. If we have structured SRV record names like &lt;code&gt;telemetry.&amp;lt;zone&amp;gt;.&amp;lt;job&amp;gt;.srv.example.org&lt;/code&gt;,
we can extract relevant labels from it those names:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
- job_name: 'myjob'

  dns_sd_configs:
  - names:
    - 'telemetry.eu-west.api.srv.example.org'
    - 'telemetry.us-west.api.srv.example.org'
    - 'telemetry.eu-west.auth.srv.example.org'
    - 'telemetry.us-east.auth.srv.example.org'

  relabel_configs:
  - source_labels: ['__meta_dns_srv_name']
    regex:         'telemetry\.(.+?)\..+?\.srv\.example\.org'
    target_label:  'zone'
    replacement:   '$1'
  - source_labels: ['__meta_dns_srv_name']
    regex:         'telemetry\..+?\.(.+?)\.srv\.example\.org'
    target_label:  'job'
    replacement:   '$1'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will attach the &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;job&lt;/code&gt; label to each target based on the SRV record
it came from.&lt;/p&gt;

&lt;h2 id="discovery-with-consul"&gt;Discovery with Consul&lt;a class="header-anchor" href="#discovery-with-consul" name="discovery-with-consul"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Service discovery via Consul is now supported natively. It can be configured by defining
access parameters for our Consul agent and a list of Consul services for which we want
to query targets.&lt;/p&gt;

&lt;p&gt;The tags of each Consul node are concatenated by a configurable separator and exposed
through the &lt;code&gt;__meta_consul_tags&lt;/code&gt; label. Various other Consul-specific meta
labels are also provided.&lt;/p&gt;

&lt;p&gt;Scraping all instances for a list of given services can be achieved with a simple
&lt;code&gt;consul_sd_config&lt;/code&gt; and relabeling rules:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
- job_name: 'overwritten-default'

  consul_sd_configs:
  - server:   '127.0.0.1:5361'
    services: ['auth', 'api', 'load-balancer', 'postgres']

  relabel_configs:
  - source_labels: ['__meta_consul_service']
    regex:         '(.*)'
    target_label:  'job'
    replacement:   '$1'
  - source_labels: ['__meta_consul_node']
    regex:         '(.*)'
    target_label:  'instance'
    replacement:   '$1'
  - source_labels: ['__meta_consul_tags']
    regex:         '(production|canary)'
    target_label:  'group'
    replacement:   '$1'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This discovers the given services from the local Consul agent.
As a result, we get metrics for four jobs (&lt;code&gt;auth&lt;/code&gt;, &lt;code&gt;api&lt;/code&gt;, &lt;code&gt;load-balancer&lt;/code&gt;, and &lt;code&gt;postgres&lt;/code&gt;). If a node
has the &lt;code&gt;production&lt;/code&gt; or &lt;code&gt;canary&lt;/code&gt; Consul tag, a respective &lt;code&gt;group&lt;/code&gt; label is assigned to the target.
Each target's &lt;code&gt;instance&lt;/code&gt; label is set to the node name provided by Consul.&lt;/p&gt;

&lt;p&gt;A full documentation of all configuration parameters for service discovery via Consul
can be found on the &lt;a href="/docs/operating/configuration##relabeling-relabel_config"&gt;Prometheus website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="custom-service-discovery"&gt;Custom service discovery&lt;a class="header-anchor" href="#custom-service-discovery" name="custom-service-discovery"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Finally, we added a file-based interface to integrate your custom service discovery or other common mechanisms
that are not yet supported out of the box.&lt;/p&gt;

&lt;p&gt;With this mechanism, Prometheus watches a set of directories or files which define target groups.
Whenever any of those files changes, a list of target groups is read from the files and scrape targets
are extracted.
It's now our job to write a small bridge program that runs as Prometheus's side-kick.
It retrieves changes from an arbitrary service discovery mechanism and writes the target information
to the watched files as lists of target groups.&lt;/p&gt;

&lt;p&gt;These files can either be in YAML:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- targets: ['10.11.150.1:7870', '10.11.150.4:7870']
  labels:
    job: 'mysql'

- targets: ['10.11.122.11:6001', '10.11.122.15:6002']
  labels:
    job: 'postgres'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;...or in JSON format:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[
  {
    "targets": ["10.11.150.1:7870", "10.11.150.4:7870"],
    "labels": {
      "job": "mysql"
    }
  },
  {
    "targets": ["10.11.122.11:6001", "10.11.122.15:6002"],
    "labels": {
      "job": "postgres"
    }
  }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now configure Prometheus to watch the &lt;code&gt;tgroups/&lt;/code&gt; directory in its working directory
for all &lt;code&gt;.json&lt;/code&gt; files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scrape_configs:
- job_name: 'overwritten-default'

  file_sd_configs:
  - names: ['tgroups/*.json']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What's missing now is a program that writes files to this directory. For the sake of this example,
let's assume we have all our instances for different jobs in a single denormalized MySQL table.
(Hint: you probably don't want to do service discovery this way.)&lt;/p&gt;

&lt;p&gt;Every 30 seconds, we read all instances from the MySQL table and write the
resulting target groups into a JSON file. Note that we do not have to keep
state whether or not any targets or their labels have changed. Prometheus will
automatically detect changes and applies them to targets without interrupting
their scrape cycles.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os, time, json

from itertools import groupby
from MySQLdb import connect


def refresh(cur):
    # Fetch all rows.
    cur.execute("SELECT address, job, zone FROM instances")

    tgs = []
    # Group all instances by their job and zone values.
    for key, vals in groupby(cur.fetchall(), key=lambda r: (r[1], r[2])):
        tgs.append({
            'labels': dict(zip(['job', 'zone'], key)),
            'targets': [t[0] for t in vals],
        })

    # Persist the target groups to disk as JSON file.
    with open('tgroups/target_groups.json.new', 'w') as f:
        json.dump(tgs, f)
        f.flush()
        os.fsync(f.fileno())

    os.rename('tgroups/target_groups.json.new', 'tgroups/target_groups.json')


if __name__ == '__main__':
    while True:
        with connect('localhost', 'root', '', 'test') as cur:
            refresh(cur)
        time.sleep(30)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While Prometheus will not apply any malformed changes to files, it is considered best practice to
update your files atomically via renaming, as we do in our example.
It is also recommended to split larger amounts of target groups into several files based on
logical grouping.&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;a class="header-anchor" href="#conclusion" name="conclusion"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;With DNS-SRV records and Consul, two major service discovery methods are now
natively supported by Prometheus. We've seen that relabeling is a powerful
approach to make use of metadata provided by service discovery mechanisms.&lt;/p&gt;

&lt;p&gt;Make sure to take a look at the new &lt;a href="/docs/operating/configuration/"&gt;configuration documentation&lt;/a&gt;
to upgrade your Prometheus setup to the new release and find out about other configuration options,
such as basic HTTP authentication and target filtering via relabeling.&lt;/p&gt;

&lt;p&gt;We provide a &lt;a href="https://github.com/prometheus/migrate/releases"&gt;migration tool&lt;/a&gt; that upgrades
your existing configuration files to the new YAML format.
For smaller configurations we recommend a manual upgrade to get familiar with the new format and
to preserve comments.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <id>tag:prometheus.io,2015-04-24:/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/</id>
    <title type="html">Prometheus Monitoring Spreads through the Internet</title>
    <published>2015-04-24T04:00:00Z</published>
    <updated>2015-04-24T04:00:00Z</updated>
    <author>
      <name>Brian Brazil</name>
      <uri>http://prometheus.io/blog/</uri>
    </author>
    <link rel="alternate" href="http://prometheus.io/blog/2015/04/24/prometheus-monitring-spreads-through-the-internet/"/>
    <content type="html">&lt;p&gt;It has been almost three months since we publicy announced Prometheus version
0.10.0, and we're now at version 0.13.1.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud"&gt;SoundCloud's announcement blog post&lt;/a&gt;
remains the best overview of the key components of Prometheus, but there has
been a lot of other online activity around Prometheus. This post will let you
catch up on anything you missed.&lt;/p&gt;

&lt;p&gt;In the future, we will use this blog to publish more articles and announcements
to help you get the most out of Prometheus.&lt;/p&gt;

&lt;h2 id="using-prometheus"&gt;Using Prometheus&lt;a class="header-anchor" href="#using-prometheus" name="using-prometheus"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Posts on how to use Prometheus comprise the majority of online content. Here
are the ones we're aware of with the part of the ecosystem they cover:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Container Exporter: &lt;a href="https://5pi.de/2015/01/26/monitor-docker-containers-with-prometheus/"&gt;Monitor Docker Containers with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;HAProxy: &lt;a href="http://www.boxever.com/haproxy-monitoring-with-prometheus"&gt;HAProxy Monitoring with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Java Client: &lt;a href="http://www.boxever.com/easy-java-instrumentation-with-prometheus"&gt;Easy Java Instrumentation with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Java Client and Labels: &lt;a href="http://www.boxever.com/the-power-of-multi-dimensional-labels-in-prometheus"&gt;The Power of Multi-Dimensional Labels in Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Node Exporter: &lt;a href="http://www.boxever.com/monitoring-your-machines-with-prometheus"&gt;Monitoring your Machines with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;JMX Exporter: &lt;a href="http://www.boxever.com/cassandra-consoles-with-jmx-and-prometheus"&gt;Cassandra Consoles with JMX and Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Python Client and Node Exporter Textfile Collector: &lt;a href="http://www.boxever.com/monitoring-python-batch-jobs"&gt;Monitoring Python Batch Jobs&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Mesos Exporter: &lt;a href="http://www.antonlindstrom.com/2015/02/24/monitoring-mesos-tasks-with-prometheus.html"&gt;Monitoring Mesos tasks with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Synapse: &lt;a href="http://matrix.org/blog/2015/04/23/monitoring-synapse-metrics-with-prometheus/"&gt;Monitoring Synapse Metrics with Prometheus&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="articles"&gt;Articles&lt;a class="header-anchor" href="#articles" name="articles"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;These articles look at how Prometheus fits into the broader picture of keeping services up and running:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.boxever.com/prometheus-a-next-generation-monitoring-system"&gt;Prometheus: A Next-Generation Monitoring System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://thenewstack.io/soundclouds-prometheus-monitoring-system-time-series-database-suited-containers/"&gt;SoundCloud’s Prometheus: A Monitoring System and Time Series Database Suited for Containers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rancher.com/docker-monitoring-continued-prometheus-and-sysdig/"&gt;Docker Monitoring Continued: Prometheus and Sysdig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="philosophy"&gt;Philosophy&lt;a class="header-anchor" href="#philosophy" name="philosophy"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Monitoring isn't just about the technical details. How it affects the design of
your systems, operations, and human factors are important too:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.boxever.com/push-vs-pull-for-monitoring"&gt;Push vs Pull for Monitoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/brianbrazil/devops-ireland-systems-monitoring-with-prometheus"&gt;Systems Monitoring with Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.slideshare.net/brianbrazil/python-ireland-monitoring-your-python-with-prometheus"&gt;Monitoring your Python with Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The comments on the &lt;a href="https://news.ycombinator.com/item?id=8995696"&gt;Hacker News post&lt;/a&gt; about Prometheus are also insightful.&lt;/p&gt;

&lt;h2 id="non-english"&gt;Non-English&lt;a class="header-anchor" href="#non-english" name="non-english"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Several posts have appeared in languages beyond English:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Japanese how-to about installing Prometheus on CentOS: &lt;a href="http://y-ken.hatenablog.com/entry/how-to-install-prometheus"&gt;データ可視化アプリの新星、PrometheusをCentOSにインストールする方法&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Japanese in-depth tutorial: &lt;a href="http://pocketstudio.jp/log3/2015/02/11/what_is_prometheus_monitoring/"&gt;【入門】PrometheusでサーバやDockerコンテナのリソース監視&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Japanese overview: &lt;a href="http://wazanova.jp/items/1672"&gt;Prometheus: Go言語で書かれたモニタリングシステム&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Russian podcast that mentions Prometheus: &lt;a href="http://www.rwpod.com/posts/2015/02/02/podcast-03-04.html"&gt;RWPOD 04 выпуск 03 сезона&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="closing"&gt;Closing&lt;a class="header-anchor" href="#closing" name="closing"&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p&gt;Finally, I'd like to share how to run &lt;a href="https://5pi.de/2015/02/10/prometheus-on-raspberry-pi/"&gt;Prometheus on a Raspberry Pi&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
</feed>

